{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Spotahome.com](https://www.spotahome.com/s/london--uk/for-rent:apartments/for-rent:studios/bedrooms:1/bedrooms:2/bedrooms:3/bedrooms:3more?areaId[]=219&areaId[]=231&areaId[]=232&areaId[]=233&areaId[]=234&areaId[]=235&areaId[]=236&areaId[]=237&areaId[]=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe with information from search result page (runtime ~1m 20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                 # requests on websites\n",
    "from bs4 import BeautifulSoup   # html parsing\n",
    "import pandas as pd             # pandas for data frame\n",
    "import math                     # for math methods\n",
    "import time                     # for sleep timer\n",
    "import random\n",
    "from sql_functions import *     # functions from file for upload on schema\n",
    "import psycopg2                 # for upload on engine\n",
    "import datetime as dt           # for the csv file with the current date and time\n",
    "import re\n",
    "\n",
    "\n",
    "def get_description(bs):\n",
    "    lst_name = []\n",
    "    descriptions = bs.find_all(\n",
    "        class_='homecard-content__title__HomecardContent___OmV4c homecard-content__title--rebranding-style__HomecardContent___OmV4c')\n",
    "    for description in descriptions:\n",
    "        lst_name.append(\n",
    "            description.get_text()\n",
    "                .strip()\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "def get_housing(bs):\n",
    "    lst_name = []\n",
    "    housings = bs.find_all(\n",
    "        class_='homecard-content__type__HomecardContent___OmV4c homecard-content__type--rebranding-style__HomecardContent___OmV4c')\n",
    "    for housing in housings:\n",
    "        lst_name.append(\n",
    "            housing.get_text()\n",
    "                .strip()\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "def get_available(bs):\n",
    "    lst_name = []\n",
    "    availables = bs.find_all(\n",
    "        class_='homecard-content__available-from__HomecardContent___OmV4c homecard-content__available-from--rebranding-style__HomecardContent___OmV4c')\n",
    "    for available in availables:\n",
    "        lst_name.append(\n",
    "            available.get_text()\n",
    "                .strip()\n",
    "                .replace('From ', '')\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "def get_price(bs):\n",
    "    lst_name = []\n",
    "    prices = bs.find_all(class_='price__Price___OmV4c')\n",
    "    for price in prices:\n",
    "        lst_name.append(\n",
    "            price.get_text()\n",
    "                .strip()\n",
    "                .replace('Â£', '')\n",
    "                .split('-')[0]\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "def get_prices_period(bs):\n",
    "    lst_name = []\n",
    "    prices_period = bs.find_all(\n",
    "        class_='price-monthly__Price___OmV4c price-monthly--rebranding-style__Price___OmV4c')\n",
    "    for price_period in prices_period:\n",
    "        lst_name.append(\n",
    "            price_period.get_text()\n",
    "                .strip()\n",
    "                .replace('/', '')\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "def get_ids(bs):\n",
    "    lst_name = []\n",
    "    ids = bs.find_all(class_='l-list__item')\n",
    "    for id in ids:\n",
    "        lst_name.append(\n",
    "            id.get('data-homecard-scroll')\n",
    "                .strip()\n",
    "        )\n",
    "    return lst_name\n",
    "\n",
    "\n",
    "# Create dictionary in which every location ID gets assigned a location name\n",
    "location_dict = {219: 'Lambeth',\n",
    "                 231: 'Hammersmith and Fulham',\n",
    "                 232: 'Kensington and Chelsea',\n",
    "                 233: 'City of Westminster',\n",
    "                 234: 'Camden',\n",
    "                 235: 'Tower Hamlets',\n",
    "                 236: 'Islington',\n",
    "                 237: 'Hackney',\n",
    "                 241: 'City of London'\n",
    "                 }\n",
    "\n",
    "\n",
    "# the website spotahome shows 60 search results per page. To iterate trough all the pages, we get the information how many search results are there, then divide it by 60 and round it up to get the number of pages.\n",
    "def page_results(property_type, location):\n",
    "    page = requests.get(\n",
    "        f'https://www.spotahome.com/s/london--uk/for-rent:{property_type}?areaId[]={location}')\n",
    "    html = page.content\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extracting the total number of search results\n",
    "    results = bs.find_all('h1', {'class': 'search-title__title'})\n",
    "\n",
    "    result_text = 0\n",
    "\n",
    "    for result in results:\n",
    "        result_text = result.find(\"strong\").get_text().strip()\n",
    "\n",
    "    # convert the extracted string to an integer to perform mathematical operations\n",
    "    result_converted = int(result_text)\n",
    "\n",
    "    # divide the converted result by 60 since one pages shows 60 results and round it up to get the number of pages\n",
    "    page_site = result_converted / 60\n",
    "    page_site = math.ceil(page_site)\n",
    "\n",
    "    # convert the number of pages from a float to an integer to iterate through the pages\n",
    "    page_converted = int(page_site)\n",
    "\n",
    "    df_search = pd.DataFrame()\n",
    "\n",
    "    begin = f'https://www.spotahome.com/s/london--uk/for-rent:{property_type}'\n",
    "    end = f'?areaId[]={location}'\n",
    "\n",
    "    # range is including in the beginning and excluding in the end so we add plus 1 to iterate through all calculated pages\n",
    "    page_converted = page_converted + 1\n",
    "\n",
    "    for page_number in range(page_converted):\n",
    "        time.sleep(random.randint(2,6)/10)\n",
    "        page = requests.get(begin+f'/page:{page_number}'+end)\n",
    "        html = page.content\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Create a dictionary to store the results from every loop cycle.\n",
    "        # The keys are the column names and the values are the functions we created before.\n",
    "        # The functions are called with the beautiful soup object as a parameter.\n",
    "        spotahome_dict = {\n",
    "            'platform_id': get_ids(bs),\n",
    "            'platform': 'spotahome',\n",
    "            'neighborhood': location_dict[location],\n",
    "            'property_type': property_type,\n",
    "            'housing_type': get_housing(bs),\n",
    "            'price_pcm': get_price(bs),\n",
    "            'title': get_description(bs),\n",
    "            'furnished': 'furnished',\n",
    "            'available_from': get_available(bs),\n",
    "        }\n",
    "        df_page = pd.DataFrame(data=spotahome_dict)\n",
    "\n",
    "        # the temporary data frame stores the data to the data frame we created earlier outside the for loop\n",
    "        # for every iteration, the data frame page stores the results in the data frame search\n",
    "        df_search = pd.concat([df_search, df_page], axis=0, ignore_index=True)\n",
    "    return (df_search)\n",
    "\n",
    "\n",
    "property_types = ['studios', 'apartments/bedrooms:1',\n",
    "                  'apartments/bedrooms:2', 'apartments/bedrooms:3', 'apartments/bedrooms:3more']\n",
    "locations = [219, 231, 232, 233, 234, 235, 236, 237, 241]\n",
    "\n",
    "df_complete = pd.DataFrame()\n",
    "for property_type in property_types:\n",
    "    for location in locations:\n",
    "        df_complete = pd.concat([df_complete, page_results(\n",
    "            property_type, location)], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_complete)\n",
    "df_complete.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe with information from every detail page for every apartment advert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_details_complete = pd.DataFrame()\n",
    "\n",
    "#with iterrows we can grab the id's from the previous code, iterate trough all of them and get the details for every id\n",
    "for idx, row in df_complete.iterrows():\n",
    "    time.sleep(random.randint(2,6)/10)\n",
    "    page = f\"https://www.spotahome.com/london/for-rent:{row['housing_type'].lower() + 's'}/{row['platform_id']}\"\n",
    "    website = requests.get(page)\n",
    "    results = BeautifulSoup(website.content, 'html.parser')\n",
    "\n",
    "    details = results.find(\n",
    "        'div', class_='property-title__details').find_all('span')\n",
    "\n",
    "    #our information is first stored in a list\n",
    "    details_lst = (detail.get_text() for detail in details)\n",
    "    details_lst = [detail.strip() for detail in details_lst]\n",
    "\n",
    "    #since we don't need the property_type again, we drop that information\n",
    "    details_lst.pop(0)\n",
    "\n",
    "    #we can split our information on the space and get three lists inside a list\n",
    "    details_lst = [i.split(' ', 1) for i in details_lst]\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    #the three lists are now get stored in a dictionary (with the id's)\n",
    "    for value_key_tuple in details_lst:\n",
    "        new_key_value = {'id': row['platform_id']}\n",
    "        row_dict.update(new_key_value)\n",
    "        key = value_key_tuple[1]\n",
    "        value = value_key_tuple[0]\n",
    "        row_dict[key] = value\n",
    "\n",
    "    #the dictionary is then converted to a data frame\n",
    "    df_details = pd.DataFrame(data=row_dict, index=[0])\n",
    "    df_details_complete = pd.concat([df_details_complete, df_details])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_details_complete\n",
    "df_details_complete.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import df_complete to DBeaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call the schema created for this project\n",
    "# schema = 'capstone_jmrs'\n",
    "# # get the function to connect to the database\n",
    "# engine = get_engine()\n",
    "\n",
    "# # give the table a unique name\n",
    "# table_name = 'spotahome_df_complete_2'\n",
    "\n",
    "# # import the table to sql\n",
    "# if engine != None:\n",
    "#     try:\n",
    "#         df_complete.to_sql(name=table_name,\n",
    "#                            con=engine,\n",
    "#                            if_exists='replace',\n",
    "#                            schema=schema,\n",
    "#                            index=False,\n",
    "#                            chunksize=5000,\n",
    "#                            method='multi')\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import df_details_complete to DBeaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call the schema created for this project\n",
    "# schema = 'capstone_jmrs'\n",
    "# # get the function to connect to the database\n",
    "# engine = get_engine()\n",
    "\n",
    "# # give the table a unique name\n",
    "# table_name = 'spotahome_df_details_complete'\n",
    "\n",
    "# # import the table to sql\n",
    "# if engine != None:\n",
    "#     try:\n",
    "#         df_details_complete.to_sql(name=table_name,\n",
    "#                                    con=engine,\n",
    "#                                    if_exists='replace',\n",
    "#                                    schema=schema,\n",
    "#                                    index=False,\n",
    "#                                    chunksize=5000,\n",
    "#                                    method='multi')\n",
    "#         print(f\"The {table_name} table was imported successfully.\")\n",
    "\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "#         engine = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_sql')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81d278bed5b5b59425dcb5a82ce505657686243c184b4a6b67e69d01c4d432e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
