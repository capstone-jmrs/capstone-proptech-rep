{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "### Web Scraping Best Practices:\n",
    "\n",
    "- Never scrape more frequently than you need to.\n",
    "- Consider caching the content you scrape so that it’s only downloaded once.\n",
    "- Build pauses into your code using functions like time.sleep() to keep from overwhelming servers with too many requests too quickly.\n",
    "- Video von [neuefische](https://www.youtube.com/watch?v=HMSe8WTNmFg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "The library we will use today to find fishes we can gift Larissa for christmas is [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). It is a library to extract data out of HTML and XML files.\n",
    "\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python requests.\n",
    "\n",
    "The requests library will make a GET request to a web server, which will download the HTML contents of a given web page for us. There are several different types of requests we can make using requests, of which GET is just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Blueground - robots.txt\n",
    "User-agent: *  <br>\n",
    "Disallow: /book <br>\n",
    "Disallow: /book-failed<br>\n",
    "Disallow: /book-thankyou<br>\n",
    "Disallow: /expired<br>\n",
    "Disallow: /feedback<br>\n",
    "Disallow: /guests<br>\n",
    "Disallow: /nps<br>\n",
    "Disallow: /offers<br>\n",
    "Disallow: /payment-failed<br>\n",
    "Disallow: /payment-thankyou<br>\n",
    "Disallow: /payments<br>\n",
    "Disallow: /rating<br>\n",
    "Disallow: /users<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/ist.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/lon.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/par.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/vie.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/dxb.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/mia.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/nyc.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/sfo.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/lax.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/bos.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/wdc.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/chi.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/sea.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/den.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/atx.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/zrh.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/ber.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/mad.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/bcn.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/lis.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/bsl.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/hkg.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/cph.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/lux.xml<br>\n",
    "Sitemap: https://www.theblueground.com/sitemap-images/sgp.xml<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the content of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the content of the website\n",
    "# Blueground - London\n",
    "# https://www.theblueground.com/furnished-apartments-london-uk\n",
    "# page = requests.get(\"https://www.theblueground.com/furnished-apartments-london-uk\")\n",
    "# html = page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 12\n",
    "page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "html = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the BeautifulSoup library to parse this document, and extract the information from it.\n",
    "\n",
    "We first have to import the library, and create an instance of the BeautifulSoup class to parse our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html and save it into a BeautifulSoup instance\n",
    "bs = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print out the HTML content of the page, formatted nicely, using the prettify method on the BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bs.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we have more than one element with the same tag? Then we can just use the ```.find_all()``` method of BeautifulSoup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for the Apartment/Studio Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the apartments\n",
    "object_names = bs.find_all(class_=\"listing-name\")\n",
    "object_names_lst = (object_name.get_text() for object_name in object_names)\n",
    "object_names_lst = [object_name.strip() for object_name in object_names_lst]\n",
    "object_names_lst[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### availabe - Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the availabe - tag\n",
    "available_tags = bs.find_all(class_=\"availability__available\")\n",
    "available_tag_lst = (available_tag.get_text() for available_tag in available_tags)\n",
    "available_tag_lst = [available_tag.strip() for available_tag in available_tag_lst]\n",
    "available_tag_lst[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### available from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the availabe from information\n",
    "availables = bs.find_all(class_=\"availability__date\")\n",
    "available_lst = (available.get_text() for available in availables)\n",
    "available_lst = [available.strip() for available in available_lst]\n",
    "available_lst[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price - currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the price__currency\n",
    "price_currencys = bs.find_all(class_= \"price__currency\")\n",
    "price_currencys_lst = [price_currency.get_text() for price_currency in price_currencys]\n",
    "price_currencys_lst = [price_currency.strip() for price_currency in price_currencys_lst]\n",
    "price_currencys_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price - amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the price amount\n",
    "prices = bs.find_all(class_= \"price__amount\")\n",
    "prices_lst = [price.get_text() for price in prices]\n",
    "prices_lst = [price.strip() for price in prices_lst]\n",
    "prices_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price per month etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the price per month tag\n",
    "prices_month = bs.find_all(class_= \"monthly-price__suffix monthly-price__suffix--mobile\")\n",
    "prices_month_lst = [price_month.get_text() for price_month in prices_month]\n",
    "prices_month_lst = [price_month.strip() for price_month in prices_month_lst]\n",
    "prices_month_lst = [price_month.replace('/', '') for price_month in prices_month_lst]\n",
    "prices_month_lst = [price_month.replace('mo', 'month') for price_month in prices_month_lst]\n",
    "prices_month_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### complett description of the appartement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the description of the apartments\n",
    "descriptions= bs.find_all(class_=\"listing-amenities\")\n",
    "descriptions_lst = [description.get_text() for description in descriptions]\n",
    "descriptions_lst = [description.strip() for description in descriptions_lst]\n",
    "descriptions_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main-amenities  of the appartement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the main-amenities of the apartments\n",
    "main_amenities= bs.find_all(class_=\"main-amenities\")\n",
    "main_amenities_lst = [main_amenitie.get_text() for main_amenitie in main_amenities]\n",
    "main_amenities_lst = [main_amenitie.strip() for main_amenitie in main_amenities_lst]\n",
    "main_amenities_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_amenities_amenity seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the main-amenities of the apartments seperated\n",
    "main_amenities_amenitys = bs.find_all(class_=\"main-amenities__amenity\")\n",
    "main_amenities_amenity_lst = [main_amenitie_amenity.get_text() for main_amenitie_amenity in main_amenities_amenitys]\n",
    "main_amenities_amenity_lst = [main_amenitie_amenity.strip() for main_amenitie_amenity in main_amenities_amenity_lst]\n",
    "main_amenities_amenity_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_amenities_amenity_lst = [main_amenitie.strip() for main_amenitie in main_amenities_amenity_lst]\n",
    "# main_amenities_amenity_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rest_amenities of apartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rest of the amenities\n",
    "rest_amenities = bs.find_all(class_=\"rest-amenities\")\n",
    "rest_amenities_lst = [rest_amenity.get_text() for rest_amenity in rest_amenities]\n",
    "rest_amenities_lst = [rest_amenity.strip() for rest_amenity in rest_amenities_lst]\n",
    "rest_amenities_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the hyperlink from the website to the detail-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get all the elements of the class \"ui-image-carousel\"\n",
    "results = bs.find_all(class_=\"ui-image-carousel\")\n",
    "# we will look for the element a\n",
    "find_a= results[0].find_all('a')\n",
    "print(find_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give us all the links of the website\n",
    "soup = bs\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    print(\"Found the URL:\", a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to find the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will get us the link to the detail page\n",
    "class_with_link = bs.find_all(class_=\"ui-image-carousel\")\n",
    "# with the [] we can select the elment we want to get\n",
    "for a in class_with_link[0].find_all('a', href=True):\n",
    "    url = a['href']\n",
    "    #print(\"Found the URL:\", a['href'])\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lst = []\n",
    "# this will get us the link to the detail page\n",
    "class_with_link = bs.find_all(class_=\"ui-image-carousel\")\n",
    "# with the [] we can select the elment we want to get\n",
    "for a in class_with_link[0].find_all('a', href=True):\n",
    "    url_lst.append(a['href'])\n",
    "    #print(\"Found the URL:\", a['href'])\n",
    "print(url_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lst = []\n",
    "\n",
    "count = df.index.max()\n",
    "\n",
    "\n",
    "# this will get us the link to the detail page\n",
    "class_with_link = bs.find_all(class_=\"ui-image-carousel\")\n",
    "# with the [] we can select the elment we want to get\n",
    "for a in class_with_link[count].find_all('a', href=True):\n",
    "    url_lst.append(a['href'])\n",
    "    #print(\"Found the URL:\", a['href'])\n",
    "print(url_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.spotahome.com/s/london--uk/for-rent:apartments/for-rent:studios/bedrooms:3?features[]=pets&noDeposit=1')\n",
    "html = page.content\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "ids = bs.find_all(class_ = 'l-list__item')\n",
    "ids_lst = [id.get('data-homecard-scroll') for id in ids]\n",
    "\n",
    "ids_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lst = []\n",
    "count = 0\n",
    "while count <= df.index.max():\n",
    "    \n",
    "    # this will get us the link to the detail page\n",
    "    class_with_link = bs.find_all(class_=\"ui-image-carousel\")\n",
    "    # with the [] we can select the elment we want to get\n",
    "    for a in class_with_link[count].find_all('a', href=True):\n",
    "        url_lst.append(a['href'])\n",
    "    count += 1\n",
    "print(url_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen eines DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['object_name'] = pd.Series(object_names_lst)\n",
    "df['available_tag'] = pd.Series(available_tag_lst)\n",
    "#df['available'] = pd.Series(available_lst)\n",
    "#df['description'] = pd.Series(descriptions_lst)\n",
    "#df['main_amenities'] = pd.Series(main_amenities_lst)\n",
    "#df['main_amenities_amenity'] =pd.Series( main_amenities_amenity_lst)\n",
    "#df['rest_amenities'] = pd.Series(rest_amenities_lst)\n",
    "#df['price_currencys'] = pd.Series(price_currencys_lst)\n",
    "#df['prices'] = pd.Series(prices_lst)\n",
    "#df['prices_month'] = pd.Series(prices_month_lst)\n",
    "df['detail_links'] = pd.Series(url_lst)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df.index\n",
    "print(index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the actual DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = dt.datetime.today().strftime('%Y-%m-%d %H:%M') # to set the date in the csv filename\n",
    "df.to_csv('blueground_{}.csv'.format(today), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information to get data in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look for 'class=\"blank-slate__criteria\"' , this will show us the last page of the infinite-scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&\n",
    "# for _ in range(7):\n",
    "#     time.sleep(3)\n",
    "#     if _ == 0:\n",
    "#         page = requests.get(link + offset=1&items=18)\n",
    "#         html = page.content\n",
    "#     else:\n",
    "#         print(link + f'/offset={_}&items=18')\n",
    "#         page = requests.get(link + f'/offset={_}&items=18')\n",
    "#         html = page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 1\n",
    "\n",
    "while bs.find_all(class_=(\"blank-slate__criteria\")) == False:\n",
    "    time.sleep(5)\n",
    "    page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "    html = page.content\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    print(weblink + f'/offset={ pagesite }&items=18')\n",
    "    pagesite += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us the correct url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 1\n",
    "\n",
    "while pagesite < 7:\n",
    "    #time.sleep(5)\n",
    "    #page = requests.get(weblink +  f'/offset={ pagesite }&items=18')\n",
    "    #html = page.content\n",
    "    #bs = BeautifulSoup(html, 'html.parser')\n",
    "    print(weblink + f'offset={ pagesite }&items=18')\n",
    "    pagesite += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website not found:\n",
    "\n",
    "https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&offset=18&items=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the content of the website\n",
    "# Blueground - London\n",
    "# https://www.theblueground.com/furnished-apartments-london-uk\n",
    "page_not_found = requests.get(\"https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&offset=15&items=18\")\n",
    "html_not_found = page_not_found.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html and save it into a BeautifulSoup instance\n",
    "bs_not_found = BeautifulSoup(html_not_found, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_slates = bs_not_found.find_all(class_=\"blank-slate__criteria\")\n",
    "\n",
    "blank_slates_lst = (blank_slate.get_text() for blank_slate in blank_slates)\n",
    "blank_slates_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_slates_lst = [blank_slate.strip() for blank_slate in blank_slates_lst]\n",
    "blank_slates_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We try to shop the loop with 'class=\"blank-slate__criteria\"' to see, if we have reached the end of the infinite-scroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with an Inifiy - Website - Scrolling\n",
    "\n",
    "https://medium.com/@harshvb7/scraping-from-a-website-with-infinite-scrolling-7e080ea8768e\n",
    "\n",
    "https://stackoverflow.com/questions/69046183/how-do-i-scrape-a-website-with-an-infinite-scroller\n",
    "\n",
    "https://stackoverflow.com/questions/64527791/scraping-an-infinite-scroll-page\n",
    "\n",
    "https://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 12\n",
    "\n",
    "# blank_slates = bs_not_found.find_all(class_=\"blank-slate__criteria\")\n",
    "# blank_slates_lst = (blank_slate.get_text() for blank_slate in blank_slates)\n",
    "# blank_slates_lst\n",
    "# blank_slates_lst = [blank_slate.strip() for blank_slate in blank_slates_lst]\n",
    "blank_slates_lst = []\n",
    "stop_loop = \"We’re sorry! We can’t seem to find any apartments that match your search.\"\n",
    "print(\"Startlink:\", weblink + f'offset={ pagesite }&items=18')\n",
    "print(\"We are looking for:\", stop_loop)\n",
    "print(type(stop_loop))\n",
    "print(\"We currently have:\", blank_slates_lst)\n",
    "print(type(blank_slates_lst))\n",
    "\n",
    "\n",
    "# https://flexiple.com/python/check-if-list-is-empty-python/\n",
    "# Solution 3: Using len() function\n",
    "# The len() function returns the number of items in a list. If the list is empty, it returns 0.\n",
    "while len(blank_slates_lst) == 0:\n",
    "    time.sleep(5)\n",
    "    page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "    html = page.content\n",
    "    bs_loop = BeautifulSoup(html, 'html.parser')\n",
    "    print(weblink + f'offset={ pagesite }&items=18')\n",
    "    pagesite += 1\n",
    "\n",
    "    blank_slates = bs_loop.find_all(class_=\"blank-slate__criteria\")\n",
    "    blank_slates_lst = (blank_slate.get_text() for blank_slate in blank_slates)\n",
    "    print(blank_slates_lst)\n",
    "    blank_slates_lst = [blank_slate.strip() for blank_slate in blank_slates_lst]\n",
    "    print(blank_slates_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#df_full = pd.DataFrame()\n",
    "#df_object = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 12\n",
    "page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "html = page.content\n",
    "\n",
    "# parse the html and save it into a BeautifulSoup instance\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# get the list of all the apartments\n",
    "object_names = bs.find_all(class_=\"listing-name\")\n",
    "object_names_lst = (object_name.get_text() for object_name in object_names)\n",
    "object_names_lst = [object_name.strip() for object_name in object_names_lst]\n",
    "print(object_names_lst[:5])\n",
    "\n",
    "df_object['object_name'] = pd.Series(object_names_lst)\n",
    "df_full = pd.concat([df_full, df_object], axis=0, ignore_index=True)\n",
    "display(df_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the Loop with a run\n",
    "\n",
    "## this code runs, not editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "\n",
    "import time # to pause the code\n",
    "import requests # to get the content of the website\n",
    "from bs4 import BeautifulSoup # to parse the html\n",
    "import re # to use regular expressions\n",
    "import pandas as pd # to use pandas\n",
    "import numpy as np # to use numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe\n",
    "df_full = pd.DataFrame()\n",
    "df_object = pd.DataFrame()\n",
    "df_search = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the link to the website\n",
    "\n",
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "\n",
    "# set up the first page to scrape\n",
    "pagesite = 10 # we set it to 10 to test the code\n",
    "\n",
    "# create an empty list to store the blank slates\n",
    "blank_slates_lst = [] \n",
    "\n",
    "# set the stop condition\n",
    "stop_loop = \"We’re sorry! We can’t seem to find any apartments that match your search.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flexiple.com/python/check-if-list-is-empty-python/\n",
    "# Solution 3: Using len() function\n",
    "# The len() function returns the number of items in a list. If the list is empty, it returns 0.\n",
    "while len(blank_slates_lst) == 0: # start and endpoint of the for-loop\n",
    "    # pause the loop for 3 seconds to reduce the load on the server\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "    # get the content of the website\n",
    "    page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "    # parse the html and save it into a BeautifulSoup instance\n",
    "    html = page.content\n",
    "\n",
    "    # parse the html and save it into a BeautifulSoup instance\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # get the list of all the apartments\n",
    "    object_names = bs.find_all(class_=\"listing-name\")\n",
    "    object_names_lst = (object_name.get_text() for object_name in object_names)\n",
    "    object_names_lst = [object_name.strip() for object_name in object_names_lst]\n",
    "    #print(object_names_lst[:5])\n",
    "\n",
    "    # create an empty dataframe to store the object names\n",
    "    df_object = pd.DataFrame()\n",
    "\n",
    "    # store the object names in the dataframe\n",
    "    df_object['object_name'] = pd.Series(object_names_lst)\n",
    "    df_full = pd.concat([df_full, df_object], axis=0, ignore_index=True)\n",
    "\n",
    "    # drop duplicates\n",
    "    df_full.drop_duplicates(subset=['object_name'], keep='first', inplace=True)\n",
    "\n",
    "    # set the number of rows to display to maximum\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    # display the dataframe from the loop\n",
    "    display(df_full)\n",
    "\n",
    "\n",
    "    # check if we reached the end of the pages\n",
    "    blank_slates = bs.find_all(class_=\"blank-slate__criteria\")\n",
    "    blank_slates_lst = (blank_slate.get_text() for blank_slate in blank_slates)\n",
    "    blank_slates_lst = [blank_slate.strip() for blank_slate in blank_slates_lst]\n",
    "    # print the list to make sure it works\n",
    "    #print(blank_slates_lst)\n",
    "\n",
    "    # increase the pagesite by 1\n",
    "    pagesite += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here you can start editing again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now try to create functions to work with the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### von Markus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html and save it into a BeautifulSoup instance\n",
    "#bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def get_object_name(bs):\n",
    "    # get the names of all the apartments\n",
    "    lst_name = []\n",
    "    object_names = bs.find_all(class_=\"listing-name\")\n",
    "    for object_name in object_names:\n",
    "        lst_name.append(\n",
    "            object_name.get_text()\n",
    "                .strip()\n",
    "        )\n",
    "    return lst_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_to_detail_page(bs, maximus):\n",
    "    url_lst = []\n",
    "    count = 0\n",
    "    while count <= int(maximus):\n",
    "        \n",
    "        # this will get us the link to the detail page\n",
    "        class_with_link = bs.find_all(class_=\"ui-image-carousel\")\n",
    "        # with the [] we can select the elment we want to get\n",
    "        for a in class_with_link[count].find_all('a', href=True):\n",
    "            url_lst.append(a['href'])\n",
    "        count += 1\n",
    "    #print(url_lst)\n",
    "    return url_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb Cell 76\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# we create a variable to store the number of rows in the dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m maximus \u001b[39m=\u001b[39m df_page\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mmax() \u001b[39m# we give it the max value of the index\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df_page[\u001b[39m'\u001b[39m\u001b[39mget_url_to_detail_page\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(get_url_to_detail_page(bs, maximus))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# we can now add the dataframe to the full dataframe\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df_search \u001b[39m=\u001b[39m df_search\u001b[39m.\u001b[39mappend(df_page, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb Cell 76\u001b[0m in \u001b[0;36mget_url_to_detail_page\u001b[0;34m(bs, maximus)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m url_lst \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(maximus):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# this will get us the link to the detail page\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     class_with_link \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mfind_all(class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mui-image-carousel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markussteinke/neuefische/capstone-ukio-rep/blueground_workbook_markus.ipynb#Y125sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# with the [] we can select the elment we want to get\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# https://flexiple.com/python/check-if-list-is-empty-python/\n",
    "# Solution 3: Using len() function\n",
    "# The len() function returns the number of items in a list. If the list is empty, it returns 0.\n",
    "while len(blank_slates_lst) == 0: # start and endpoint of the for-loop\n",
    "    # pause the loop for 3 seconds to reduce the load on the server\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "    # get the content of the website\n",
    "    page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "    # parse the html and save it into a BeautifulSoup instance\n",
    "    html = page.content\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # create a pandas dataframe for the names and prices\n",
    "    blueground_dict = {\n",
    "        'object_names': get_object_name(bs),\n",
    "        }\n",
    "\n",
    "    # we now have a dataframe, we can use this to get a counter for the URL\n",
    "    df_page = pd.DataFrame(blueground_dict)\n",
    "\n",
    "    # we create a variable to store the number of rows in the dataframe\n",
    "    maximus = df_page.index.max() # we give it the max value of the index\n",
    "    df_page['get_url_to_detail_page'] = pd.Series(get_url_to_detail_page(bs, maximus))\n",
    "    # we can now add the dataframe to the full dataframe\n",
    "    df_search = df_search.append(df_page, ignore_index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # # store the object names in the dataframe\n",
    "    # df_object['object_name'] = pd.Series(object_names_lst)\n",
    "    # df_full = pd.concat([df_full, df_object], axis=0, ignore_index=True)\n",
    "\n",
    "    # # drop duplicates\n",
    "    # df_full.drop_duplicates(subset=['object_name'], keep='first', inplace=True)\n",
    "\n",
    "    # # set the number of rows to display to maximum\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "\n",
    "    # # display the dataframe from the loop\n",
    "    # display(df_full)\n",
    "\n",
    "\n",
    "    # check if we reached the end of the pages\n",
    "    blank_slates = bs.find_all(class_=\"blank-slate__criteria\")\n",
    "    blank_slates_lst = (blank_slate.get_text() for blank_slate in blank_slates)\n",
    "    blank_slates_lst = [blank_slate.strip() for blank_slate in blank_slates_lst]\n",
    "    # print the list to make sure it works\n",
    "    #print(blank_slates_lst)\n",
    "\n",
    "    # increase the pagesite by 1\n",
    "    pagesite += 1\n",
    " \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "display(df_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&offset=12&items=18\n"
     ]
    }
   ],
   "source": [
    "weblink = 'https://www.theblueground.com/furnished-apartments-london-uk?currency=GBP&language=en&'\n",
    "pagesite = 12\n",
    "page = requests.get(weblink +  f'offset={ pagesite }&items=18')\n",
    "html = page.content\n",
    "print(weblink +  f'offset={ pagesite }&items=18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html and save it into a BeautifulSoup instance\n",
    "bs = BeautifulSoup(html, 'html.parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_bs4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af5746f41ef31dea01f09ffb6ba9bd116b901566dc69a02f72533c94f316243"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
